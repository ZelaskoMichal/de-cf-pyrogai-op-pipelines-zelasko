{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2abd632-3811-4125-8ec5-ca8d454601c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Set variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f4996f-d8b5-4319-9182-24d51001cbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Please set variables required to execute the script and to clone relevant tables to the backup storage account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8fc421-942f-4dbe-88a5-8f8ce1f7ff92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Script mode - 'sync' or 'failover'\n",
    "script_mode = 'sync'\n",
    "\n",
    "# Name of the backup storage account where files will be saved\n",
    "storage_account_name = '<adls_storage_account_name>'\n",
    "\n",
    "# Name of the container where cloned delta table's files will be stored\n",
    "container_name = '<container_name>'\n",
    "\n",
    "# List of databases to clone tables from\n",
    "databases_to_clone = [] # List of strings\n",
    "\n",
    "# List of single tables to clone\n",
    "tables_to_clone = [] # If provided, the function will clone only these tables\n",
    "\n",
    "# Databricks secret scope's name\n",
    "secret_scope = 'default'\n",
    "\n",
    "# ADLS connection string secret\n",
    "adls_conn_str = 'ADLS-CONN-STR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938fdb44-5688-429d-b095-089f5bffe8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb13a86-ca05-4055-97d4-41c1739c0812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Init connection to ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03f4a7e-bb8d-4365-819a-e6f69333c58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Default secret scope for all AIF DBR workspaces is called `default`. If you are using different scope, please change the variable above. The same applies to connection string stored in secrets. Default name is `ADLS-CONN-STR` and it can be edited in variables section above.\n",
    "\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope=secret_scope, key=adls_conn_str)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60881c70-6352-4af6-8596-ad68afb4f627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d7053c-0b4e-49d8-82b5-25a106155482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def get_tables_to_clone(databases_to_clone: list = None, tables_to_clone: list = None) -> list:\n",
    "    \"\"\"  \n",
    "    Returns a list of existing tables from all or selected databases in a Spark environment\n",
    "    or single tables provided in the variables section.\n",
    "  \n",
    "    Args:  \n",
    "        databases_to_clone (list): List of database names from which to retrieve tables.\n",
    "          If not provided, the function retrieves tables from all databases.\n",
    "        tables_to_clone (list): List of tables to clone. \n",
    "            If provided, rest of tables from DB will be skipped.\n",
    "  \n",
    "    Returns:  \n",
    "        list: List of tables with their databases.\n",
    "          Each table is represented as a string in the format 'databaseName.tableName'.  \n",
    "  \n",
    "    Example:  \n",
    "        >>> databases_to_clone = ['db1', 'db2']  \n",
    "        >>> get_tables_to_clone(databases_to_clone)  \n",
    "        ['db1.table1', 'db1.table2', 'db2.table1', 'db2.table2']  \n",
    "    \"\"\"\n",
    "    if databases_to_clone:  \n",
    "        databases = databases_to_clone  \n",
    "    else:  \n",
    "        databases = [  \n",
    "            db.databaseName  \n",
    "            for db in spark.sql('show databases').collect()  \n",
    "            if not db.databaseName.endswith('_clone')  \n",
    "        ]  \n",
    "  \n",
    "    tables = []  \n",
    "    for db in databases:  \n",
    "        df = spark.sql(f\"SHOW TABLES IN {db}\")  \n",
    "        df_filtered = df.filter(~df.isTemporary & df.tableName.isin(tables_to_clone if tables_to_clone else [])).select(  \n",
    "            F.concat(df.database, F.lit(\".\"), df.tableName).alias('combined')  \n",
    "        )  \n",
    "  \n",
    "        tables_in_db = df_filtered.toPandas()['combined'].tolist()  \n",
    "        tables.extend(tables_in_db)  \n",
    "  \n",
    "    return tables  \n",
    "\n",
    "\n",
    "def clone_tables(adls_path: str, tables_to_clone: list, script_mode: str) -> None:  \n",
    "    \"\"\"  \n",
    "    Clones tables from one database to another in a spark environment.  \n",
    "      \n",
    "    The function iterates over a list of tables, creates a new database if it does not exist, and then clones each table to the new database.\n",
    "  \n",
    "    Args:  \n",
    "        adls_path (str): The path where the cloned tables will be stored.  \n",
    "        tables_to_clone (list): List of tables to clone. Each table is represented as a string in the format 'databaseName.tableName'.  \n",
    "        script_mode (str): The mode of operation, 'sync' for cloning to a new database, 'failover' for cloning to the original database.  \n",
    "  \n",
    "    Returns:  \n",
    "        None\n",
    "    \"\"\"  \n",
    "    if not isinstance(adls_path, str) or not isinstance(tables_to_clone, list) or not isinstance(script_mode, str):  \n",
    "        raise ValueError(\"Invalid input parameters\")  \n",
    "          \n",
    "    for table in tables_to_clone:  \n",
    "        try:  \n",
    "            db_name, table_name = table.split(\".\")  \n",
    "            db_name_to_create = f\"{db_name}_clone\" if script_mode == 'sync' else db_name.split(\"_clone\")[0]  \n",
    "            table_path = f\"{adls_path}{db_name_to_create}.{table_name}\"  \n",
    "\n",
    "            print(f\"Creating database `{db_name_to_create}`...\")  \n",
    "            sql(f\"CREATE DATABASE IF NOT EXISTS {db_name_to_create}\")  \n",
    "\n",
    "            print(f\"Creating table `{db_name_to_create}.{table_name}` based on `{db_name}.{table_name}`...\")  \n",
    "            sql(  \n",
    "                f\"\"\"  \n",
    "                CREATE OR REPLACE TABLE {db_name_to_create}.{table_name}  \n",
    "                DEEP CLONE {db_name}.{table_name}  \n",
    "                LOCATION '{table_path}';  \n",
    "                \"\"\"  \n",
    "            )  \n",
    "        except Exception as e:  \n",
    "            print(f\"Error while cloning table `{table}`: {str(e)}\")  \n",
    "            continue  \n",
    "    print(\"All tables cloned successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6e3fcb-d4d4-4266-b395-308a1f1e2efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Clone tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42aabb60-192d-46bb-9442-8513a7e1a976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating database `delta_base_2_clone`...\nCreating table `delta_base_2_clone.table_x` based on `delta_base_2.table_x`...\nAll tables cloned successfully.\n"
     ]
    }
   ],
   "source": [
    "# Path to relevant container in storage account\n",
    "adls_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "# List of tables to clone\n",
    "tables_to_clone = get_tables_to_clone(databases_to_clone=databases_to_clone, tables_to_clone=tables_to_clone)\n",
    "\n",
    "# Execute cloning process\n",
    "clone_tables(adls_path=adls_path, tables_to_clone=tables_to_clone, script_mode=script_mode)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "delta_table_sync",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
