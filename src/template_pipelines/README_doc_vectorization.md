# Doc Vectorization â€“ Template Pipeline - Beta version

The `doc_vectorization` template pipeline demonstrates a general Generative AI use case for processing, vectorizing and storing documents in a vector database, optimized for Retrieval-Augmented Generation (RAG) applications. This template pipeline provides a simplified version of an [AskPG preprocessing pipeline](https://github.com/procter-gamble/ds-cf-askpgpipelines/tree/main/src/askpgpipelines/config). Specifically, it reads Markdown documents from Blob Storage, divides them into chunks and generates metadata for each. Then, it creates and vectorizes questions for each chunk, stores the resulting vectors in a vector database and evaluates the output. 

While the steps' content in this example is specific to the use case at hand, the pipeline's structure and the use of Azure OpenAI models are generically re-applicable to most GenAI use cases. We recommend using this example as a skeleton for your GenAI use cases by replacing the code in each step and the parameters defined in your configuration file. **Currently, the `doc_vectorization` pipeline is limited to local execution and AML. This capability will be later extended to remaining cloud providers.**

Please note that this pipeline is in its beta version.

## Files

This template pipeline adds the following files to your repository:

- `config/pipeline_doc_vectorization.yml`: Creates a typical GenAI pipeline with the following steps:
  - `ingestion` (`steps/doc_vectorization/ingestion.py`): Reads Markdown documents from blob storage, chunks them, and creates metadata for each chunk. Note that this step uses [ioslots](https://developerportal.pg.com/docs/default/Component/PyrogAI/io_overview/).
  - `vectorization` (`steps/doc_vectorization/vectorization.py`): Generates questions for each document chunk, vectorizes them and stores the results in a vector database.
  - `evaluation` (`steps/doc_vectorization/evaluation.py`): Generates questions from a set of test documents and then evaluates the vector database's search performance.
- `config/config_doc-vectorization.json`: configuration parameters for the pipeline
- `reqs/requirements_doc_vectorization.txt`: package requirements for the pipeline
- `utils/doc_vectorization/base_genai_step.py`: a customized base step for using OpenAI and Pyrogai functionalities
- `utils/doc_vectorization/toolkit.py`: utilities for working with GenAI/OpenAI LLMs

## Configuration Parameters

The `config_doc-vectorization.json` file contains the following keys:

- `doc_vectorization`, with the following subkeys:
  - `data_dir`: Folder which contains the input data, relative to the base path implied by the provider
  - `genai_proxy`: GenAI platform proxy
  - `cognitive_services`: Set of OpenAI or GenAI platform APIs to interact with
  - `open_api_version`: Particular release of Open API
  - `headers`: Additional information to be sent along with a request to the GenAI platform
    - `userid`: @pg.com email address of a person utilizing the GenAI platform
    - `project-name`: Project name should match one from the itaccess group created for your project
  - `llm_params`: Parameters for configuring the LLM used in the vectorization process
    - `deployment_name`: OpenAI chat model
    - `temperature`: A parameter that controls randomness of texts generated by a chat model
    - `streaming`: Enables or disables streaming of model outputs
    - `request_timeout`: Maximum time, in seconds, to wait for an LLM request to complete
    - `model_kwargs`: Additional model-specific parameters
        - `top_p`: A parameter used to control the diversity of generated responses by limiting the set of possible words or tokens the model considers when generating each output
  - `embedding_params`: Parameters for configuring document embeddings used in the vectorization process
    - `deployment`: OpenAI embedding model
    - `chunk_size`: Number of document chunks to process in each embedding request
    - `request_timeout`: Maximum time, in seconds, to wait for an embedding request to complete
    - `max_retries`: Maximum number of retries if the embedding request fails
    - `show_progress_bar`: Displays a progress bar during embedding requests if set to true

## Secrets
If you want to run the `doc_vectorization` template pipeline locally using your pg account, skip this section and jump to the next one on how to grant [access to the GenAI platform](#access-to-the-genai-platform).

For running the `doc_vectorization` template pipeline locally or on your AML platform using service principal credentials, some additional secret configuration is required. If you have created your project using AI Provisioner, you should have your service principal credentials (AML-APP-SP-ID and AML-APP-SP-SECRET) stored in an appropriate Azure keyvault. If not, upload the service principal credentials to the keyvault.

To be able to run the `doc_vectorization` template pipeline locally using the service principal, the following secrets need to be added to secrets.json in your config module.
```sh
{
  "AML-APP-SP-ID": "service principal id",
  "AML-APP-SP-SECRET": "service principal secret",
  "tenant-id": "azure tenant id"
}
```
To be able to run the `doc_vectorization` template pipeline on AML, upload the values in secrets.json to the Azure keyvault.

## Access to the GenAI Platform

In order to take full advantage of Generative AI capabilities and services, a required access to the GenAI platform is needed. See [how-to get your access to GenAI](https://developerportal.pg.com/docs/default/Component/genAI-Platform/access/).

## Usage

**Important:** Open all the steps and files created with the template, and modify them based on your needs, before you attempt to run the pipeline.

### Install requirements

After importing this doc_vectorization pipeline, find the relative path to requirements_doc_vectorization.txt and install all the necessary dependencies:

```sh
pip install -r <RELATIVE_PATH>/requirements_doc_vectorization.txt
```

### Parameters

This pipeline does not have any runtime parameters. To add runtime parameters, see [pipeline creation](https://developerportal.pg.com/docs/default/component/pyrogai/aif.pyrogai.pipelines.models.pipeline/) and [run time parameters in steps](https://developerportal.pg.com/docs/default/component/pyrogai/aif.pyrogai.steps.step/#aif.pyrogai.steps.step--runtime-parameters).


### Datasets setup
Download Markdown documents from [the sharepoint site](https://pgone.sharepoint.com/sites/AIFUserFiles/Tutorial%20Data/Forms/AllItems.aspx?id=%2Fsites%2FAIFUserFiles%2FTutorial%20Data%2FDocs&viewid=5510c1e4%2D1bc7%2D4f0f%2D8bee%2D93bc5b9ba294).
- For local pipeline runs, save the md files in the root project directory under `docs/inputs`.
- For AML pipeline runs, upload the `docs/inputs` folder containing the downloaded data to Azure Blob Storage defined in provider_aml.yml in your config module.

If the `docs/inputs` folder does not exist, create one. Make sure the `docs` folder name matches the value of `data_dir` in `config_doc-vectorization.json`.

### Unit Testing in Your Project
When you pull your project's pipeline, you'll find the tests under the `src/<your project name>/tests/` directory. There are two main approaches to creating your own unit tests:
- Using Pyrogai Mock from the Pyrogai library
- Creating your own mock

**Approach 1: Using Pyrogai Mock**

This is the recommended method for most cases.
For documentation, refer to: [Pyrogai Mock Documentation](https://developerportal.pg.com/docs/default/Component/PyrogAI/test_mock_step/)
To see implementation examples, you can pull the `ml_iris` or `ml_skeleton` pipelines.

**Approach 2: Creating Your Own Mock**

If you prefer this method, refer to the test examples that came with your pipeline.

To get started, pull your project's pipeline and navigate to the tests directory. Choose the approach that best suits your needs and refer to the provided examples for guidance.

### How to run example

```bash
# Run locally
aif pipeline run --pipelines doc_vectorization --environment dev
```

```bash
# Run on AML platform
aif pipeline run --pipelines doc_vectorization --environment dev --platform AML
```

### Extra configuration (Optional)
If the `doc_vectorization` template pipeline is not able to access the GenAI platform from AML, some private endpoint between AML and AKS (in which the GenAI platform is deployed) needs to be created. Reach out to the Azure platform team for support on this matter.